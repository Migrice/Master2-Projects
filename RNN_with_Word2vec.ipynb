{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccdf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load librairies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8bb0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load function for word2vec\n",
    "from utils import SentenceEmbedding, Dataset_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e79f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-trained word embedding\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f33236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load datas\n",
    "data = pd.read_csv(\"Twitter Sentiments.csv\")\n",
    "df = data.head(1000)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00de6105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq 27\n"
     ]
    }
   ],
   "source": [
    "#get the maximum sequence length\n",
    "token_lens = []\n",
    "for seq in df[\"tweet\"]:\n",
    "    token_lens.append(len(gensim.utils.simple_preprocess(seq)))\n",
    "    \n",
    "print(\"max_seq\",max(token_lens))\n",
    "maximum_seq_length = max(token_lens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc23292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appy word2vec embedding to the dataset\n",
    "data_preproceed = Dataset_embedding(df[\"tweet\"], model,maximum_seq_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4e233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 27, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproceed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66f7609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 09:52:16.522943: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-05 09:52:16.522991: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-05 09:52:16.523031: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (emelda-Latitude-E5420): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "#LSTM model\n",
    "\n",
    "lstm_out = 512\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b582e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "#encode label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"label\"])\n",
    "print(\"ft\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6eeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide in train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_preproceed, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ae033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "80/80 [==============================] - 45s 486ms/step - loss: 0.3031 - accuracy: 0.0725 - val_loss: 0.2414 - val_accuracy: 0.0850\n",
      "Epoch 2/8\n",
      "80/80 [==============================] - 41s 511ms/step - loss: 0.2064 - accuracy: 0.0725 - val_loss: 0.1942 - val_accuracy: 0.0850\n",
      "Epoch 3/8\n",
      "80/80 [==============================] - 41s 519ms/step - loss: 0.1826 - accuracy: 0.0725 - val_loss: 0.1650 - val_accuracy: 0.0850\n",
      "Epoch 4/8\n",
      "80/80 [==============================] - 43s 532ms/step - loss: 0.1503 - accuracy: 0.0725 - val_loss: 0.1716 - val_accuracy: 0.0850\n",
      "Epoch 5/8\n",
      "80/80 [==============================] - 44s 553ms/step - loss: 0.1102 - accuracy: 0.0725 - val_loss: 0.1605 - val_accuracy: 0.0850\n",
      "Epoch 6/8\n",
      "80/80 [==============================] - 47s 584ms/step - loss: 0.0875 - accuracy: 0.0725 - val_loss: 0.2109 - val_accuracy: 0.0850\n",
      "Epoch 7/8\n",
      "80/80 [==============================] - 45s 557ms/step - loss: 0.0868 - accuracy: 0.0725 - val_loss: 0.1827 - val_accuracy: 0.0850\n",
      "Epoch 8/8\n",
      "80/80 [==============================] - 46s 573ms/step - loss: 0.1973 - accuracy: 0.0725 - val_loss: 0.2408 - val_accuracy: 0.0850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2c5e68df0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 8, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f95d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 154ms/step - loss: 0.2408 - accuracy: 0.0850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2408476024866104, 0.08500000089406967]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dc104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
